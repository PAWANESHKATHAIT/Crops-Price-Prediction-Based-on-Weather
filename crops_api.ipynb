{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base API URL and API key\n",
    "api_url = \"https://api.data.gov.in/resource/9ef84268-d588-465a-a308-a864a43d0070\"\n",
    "api_key = \"579b464db66ec23bdd0000011e280179f9e34e0665f3ecd47a0e7915\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial parameters for fetching data\n",
    "limit = 1000  # Number of rows to fetch at a time\n",
    "offset = 0    # Start point for data fetching\n",
    "all_data = [] # List to store all data chunks\n",
    "rows_to_fetch = 20000  # Total rows to fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch 20,000 rows (20 batches of 1,000 rows)\n",
    "while len(all_data) * limit < rows_to_fetch:\n",
    "    # Update the API URL with the current offset and limit\n",
    "    url = f\"{api_url}?api-key={api_key}&format=csv&limit={limit}&offset={offset}\"\n",
    "    \n",
    "    # Request data\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Break the loop if the request fails or if there is no data left to fetch\n",
    "    if response.status_code != 200 or len(response.text) < 100:\n",
    "        break\n",
    "    \n",
    "    # Read data into DataFrame directly from the response content\n",
    "    batch_data = pd.read_csv(url)\n",
    "    all_data.append(batch_data)  # Append to list\n",
    "    \n",
    "    # Increase the offset to fetch the next batch\n",
    "    offset += limit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data saved to crop_data_full.csv\n"
     ]
    }
   ],
   "source": [
    "# Concatenate all batches into a single DataFrame\n",
    "full_data = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Ensure we have exactly 20,000 rows (if we fetched more than needed)\n",
    "full_data = full_data.head(rows_to_fetch)\n",
    "\n",
    "# Save the full data to a CSV file\n",
    "full_data.to_csv(\"crop_data_full.csv\", index=False)\n",
    "print(\"All data saved to crop_data_full.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
